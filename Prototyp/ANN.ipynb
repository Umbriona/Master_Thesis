{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import scipy.misc\n",
    "#from utils import *\n",
    "import pandas as pd\n",
    "\n",
    "# Molecule data\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Discriminator\n",
    "\n",
    "def discriminator(input,layers_dim, is_train, reuse=False):\n",
    "    \n",
    "    with tf.variable_scope('dis') as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "                \n",
    "        # Input layer (layers_dim[0]) encoding layer\n",
    "        w1 = tf.get_variable('w1_dis', shape=[layers_dim[2], layers_dim[2]-3], dtype=tf.float32,\n",
    "                             initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        b1 = tf.get_variable('b1_dis', shape=[layers_dim[2]-3], dtype=tf.float32,\n",
    "                             initializer=tf.constant_initializer(0.0))\n",
    "        dense1 = lrelu(tf.add(tf.matmul(input, w1), b1, name='dis_mat_mul_1'), 'discriminator_input_layer', leak = 0.2)\n",
    "        bn1 = tf.contrib.layers.batch_norm(dense1, is_training = is_train, epsilon=1e-5, decay = 0.9,\n",
    "                                           updates_collections=None, scope = 'bn1_dis')  \n",
    "\n",
    "        # First hidedn layer (layers_dim[1]) encoding layer\n",
    "        w2 = tf.get_variable('w2_dis', shape=[layers_dim[2]-3, 5], dtype=tf.float32,\n",
    "                             initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        b2 = tf.get_variable('b2_dis', shape=[5], dtype=tf.float32,\n",
    "                             initializer=tf.constant_initializer(0.0))\n",
    "        dense2 = lrelu(tf.add(tf.matmul(bn1, w2), b2, name='dis_mat_mul_2'), 'encoder_hlayer', leak = 0.2)\n",
    "        bn2 = tf.contrib.layers.batch_norm(dense2, is_training = is_train, epsilon=1e-5, decay = 0.9,\n",
    "                                           updates_collections=None, scope = 'bn2_dis')\n",
    "\n",
    "\n",
    "        w3 = tf.get_variable('w3_dis', shape=[5, 1], dtype=tf.float32,\n",
    "                             initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        b3 = tf.get_variable('b3_dis', shape=[1], dtype=tf.float32,\n",
    "                             initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "        # wgan just get rid of the sigmoid\n",
    "        logits = tf.add(tf.matmul(bn2, w3), b3, name='logits')\n",
    "        # dcgan\n",
    "        acted_out = tf.nn.sigmoid(logits)\n",
    "        return logits #, acted_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trainer\n",
    "\n",
    "def train():\n",
    "    continue_training = False\n",
    "    path = '/mnt/SSD1/data/LD50/LD50_training_data/'\n",
    "    file_name = 'training_data_LD50_ht.txt'\n",
    "    batch_size = 128\n",
    "    fps_dim = 512\n",
    "    layers_dim = np.array([256, 64, 10])\n",
    "    fps = data_extract(csv_name = 'LD50_Test_data.csv')\n",
    "    real = np.ones([batch_size,1])\n",
    "    fake = np.zeros([batch_size,1])\n",
    "    val_lr_enc = 2e-6\n",
    "    val_lr_dec = 2e-6\n",
    "    val_lr_dis = 2e-6\n",
    "    \n",
    "    with tf.variable_scope('input'):\n",
    "        #real and fake image placholders\n",
    "        real_fps = tf.placeholder(tf.float32, shape = [None, fps_dim], name='real_fps')\n",
    "        gen_fps = tf.placeholder(tf.float32, shape=[None, fps_dim], name='gen_fps')\n",
    "        dist_encode = tf.placeholder(tf.float32, shape=[None, layers_dim[2]], name='gen_fps')\n",
    "        \n",
    "        real_lable = tf.placeholder(tf.float32, shape=[None, 1], name='real_lable')\n",
    "        fake_lable = tf.placeholder(tf.float32, shape=[None, 1], name ='fake_lable')\n",
    "        \n",
    "        is_train_enc = tf.placeholder(tf.bool, name='is_train_enc')\n",
    "        is_train_dec = tf.placeholder(tf.bool, name= 'is_tain_dec')\n",
    "        is_train_dis = tf.placeholder(tf.bool, name= 'is_train_dis')\n",
    "        \n",
    "        lr_enc = tf.placeholder(tf.float32, shape = [1], name='learning_rate_encoder')\n",
    "        lr_dec = tf.placeholder(tf.float32, shape = [1], name='learning_rate_decoder')\n",
    "        lr_dis = tf.placeholder(tf.float32, shape = [1], name='learning_rate_discriminator')\n",
    "    \n",
    "    # wgan\n",
    "    real_encode = encoder(real_fps, fps_dim, layers_dim, is_train_enc, reuse=False)\n",
    "    print(real_encode.shape)\n",
    "    real_decode = decoder(real_encode, fps_dim, layers_dim, is_train_dec, reuse= False)\n",
    "    \n",
    "    \n",
    "    real_result = discriminator(dist_encode,layers_dim, is_train_dis, reuse =False)\n",
    "    fake_result = discriminator(real_encode, layers_dim, is_train_dis, reuse=True)\n",
    "    \n",
    "    dis_loss_real = tf.losses.mean_squared_error(real_result, real_lable)\n",
    "    dis_loss_fake = tf.losses.mean_squared_error(fake_result, fake_lable)\n",
    "    dis_loss = dis_loss_real + dis_loss_fake\n",
    "    #dis_loss = tf.reduce_mean(fake_result) - tf.reduce_mean(real_result)  # This optimizes the discriminator.\n",
    "    enc_loss = -tf.reduce_mean(fake_result)  # This optimizes the generator.\n",
    "    dec_loss = tf.reduce_mean(tf.square(real_fps - real_decode))        \n",
    "\n",
    "    t_vars = tf.trainable_variables()\n",
    "    \n",
    "    dis_vars = [var for var in t_vars if 'dis' in var.name]\n",
    "    enc_vars = [var for var in t_vars if 'encode' in var.name]\n",
    "    dec_vars = [var for var in t_vars if 'decode' in var.name]\n",
    "    \n",
    "    trainer_dis = tf.train.AdamOptimizer(learning_rate =lr_dis, beta1=0.9, beta2=0.999,\n",
    "                                         epsilon=1e-08, use_locking=False,\n",
    "                                         name='Adam_discriminator').minimize(dis_loss,var_list=dis_vars)\n",
    "    trainer_dec = tf.train.AdamOptimizer(learning_rate =lr_dec, beta1=0.9, beta2=0.999,\n",
    "                                         epsilon=1e-08, use_locking=False,\n",
    "                                         name='Adam_decoder').minimize(dec_loss, var_list=dec_vars)\n",
    "    trainer_enc = tf.train.AdamOptimizer(learning_rate =lr_enc, beta1=0.9, beta2=0.999,\n",
    "                                         epsilon=1e-08, use_locking=False,\n",
    "                                         name='Adam_encoder').minimize(enc_loss, var_list=enc_vars)\n",
    "    \n",
    "    \n",
    "    # clip discriminator weights\n",
    "    d_clip = [v.assign(tf.clip_by_value(v, -0.01, 0.01)) for v in dis_vars]\n",
    "\n",
    "    \n",
    "    \n",
    "    fps_batch, batch_num = process_data_np(fps, batch_size = batch_size)\n",
    "    \n",
    "    \n",
    "    total_batch = 0\n",
    "    sess = tf.Session()\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    # continue training\n",
    "    if continue_training:\n",
    "        save_path = saver.save(sess, \"/mnt/HDD1/models/LD50/model.ckpt\")\n",
    "        ckpt = tf.train.latest_checkpoint('/mnt/HDD1/models/LD50')\n",
    "        saver.restore(sess, save_path)\n",
    "\n",
    "    gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "    session_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False,\n",
    "                                    gpu_options=gpu_options)\n",
    "    #threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "\n",
    "    print('batch size: %d, batch num per epoch: %d, epoch num: %d' % (batch_size, batch_num, 5000))\n",
    "    print('start training...')\n",
    "    for i in range(51):\n",
    "        train_fps = sess.run(fps_batch)\n",
    "        print(\"Epoch %d\" % i)\n",
    "        for j in range(batch_num):\n",
    "            dis_iters = 5\n",
    "            enc_iters = 1\n",
    "            dec_iters = 1\n",
    "            \n",
    "            \n",
    "            for k in range(dis_iters):\n",
    "                \n",
    "                #wgan clip weights\n",
    "                sess.run(d_clip)\n",
    "                train_noise = np.random.normal(loc = 0, scale = 0.3,\n",
    "                                               size=[batch_size, layers_dim[2]]).astype(np.float32)\n",
    "                # Update the discriminator\n",
    "                _, disLoss = sess.run([trainer_dis, dis_loss],\n",
    "                                    feed_dict={real_fps: train_fps[j],\n",
    "                                               dist_encode: train_noise,\n",
    "                                               real_lable: real,\n",
    "                                               fake_lable: fake,\n",
    "                                               is_train_enc: False,\n",
    "                                               is_train_dis: True})\n",
    "\n",
    "            # Update the encoder\n",
    "            for k in range(enc_iters):\n",
    "                # train_noise = np.random.uniform(-1.0, 1.0, size=[batch_size, random_dim]).astype(np.float32)\n",
    "                _, encLoss = sess.run([trainer_enc, enc_loss],\n",
    "                                    feed_dict={real_fps: train_fps[j], is_train: True})\n",
    "\n",
    "            # print 'train:[%d/%d],d_loss:%f,g_loss:%f' % (i, j, dLoss, gLoss)\n",
    "            for k in range(dec_iters):\n",
    "                # train_noise = np.random.uniform(-1.0, 1.0, size=[batch_size, random_dim]).astype(np.float32)\n",
    "                _, decLoss = sess.run([trainer_dec, dec_loss],\n",
    "                                feed_dict={real_fps: train_fps[j], is_train: True})\n",
    "\n",
    "        # save check point every 500 epoch\n",
    "        if i%500 == 0:\n",
    "            if continue_training:\n",
    "                if not os.path.exists('/mnt/HDD1/models/LD50/' ):\n",
    "                    os.makedirs('/mnt/HDD1/models/LD50/')\n",
    "                saver.save(sess, '/mnt/HDD1/models/LD50/' + 'model' + str(i) + '.ckpt')  \n",
    "        if i%5 == 0:\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "            if not os.path.isfile(path + file_name):\n",
    "                fo = open(path +file_name, 'w+')\n",
    "            else:\n",
    "                fo = open(path + file_name,'a')\n",
    "             \n",
    "            \n",
    "            fo.write('%d;%f;%f;%f\\n' % (i, disLoss,  encLoss,  decLoss))\n",
    "            fo.close()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## similarity test \n",
    "def sim_test():\n",
    "    random_dim \n",
    "    with tf.variable_scope('input'):\n",
    "        real_image = tf.placeholder(tf.float32, shape = [None, HEIGHT, WIDTH, CHANNEL], name='real_image')\n",
    "        random_input = tf.placeholder(tf.float32, shape=[None, random_dim], name='rand_input')\n",
    "        is_train = tf.placeholder(tf.bool, name='is_train')\n",
    "    \n",
    "    # wgan\n",
    "    fake_image = generator(random_input, random_dim, is_train)\n",
    "    real_result = discriminator(real_image, is_train)\n",
    "    fake_result = discriminator(fake_image, is_train, reuse=True)\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    variables_to_restore = slim.get_variables_to_restore(include=['gen'])\n",
    "    print(variables_to_restore)\n",
    "    saver = tf.train.Saver(variables_to_restore)\n",
    "    ckpt = tf.train.latest_checkpoint('./model/' + version)\n",
    "    saver.restore(sess, ckpt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<rdkit.Chem.rdchem.Mol object at 0x7f121d455df0>\n",
      "(?, 10)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape must be rank 0 but is rank 1 for 'Adam_discriminator/update_dis/w1_dis/ApplyAdam' (op: 'ApplyAdam') with input shapes: [10,7], [10,7], [10,7], [], [], [1], [], [], [], [10,7].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tenv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1628\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1629\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Shape must be rank 0 but is rank 1 for 'Adam_discriminator/update_dis/w1_dis/ApplyAdam' (op: 'ApplyAdam') with input shapes: [10,7], [10,7], [10,7], [], [], [1], [], [], [], [10,7].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-0f2667b426da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-29ea870e41db>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m     trainer_dis = tf.train.AdamOptimizer(learning_rate =lr_dis, beta1=0.9, beta2=0.999,\n\u001b[1;32m     57\u001b[0m                                          \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-08\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                                          name='Adam_discriminator').minimize(dis_loss,var_list=dis_vars)\n\u001b[0m\u001b[1;32m     59\u001b[0m     trainer_dec = tf.train.AdamOptimizer(learning_rate =lr_dec, beta1=0.9, beta2=0.999,\n\u001b[1;32m     60\u001b[0m                                          \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-08\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tenv/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     return self.apply_gradients(grads_and_vars, global_step=global_step,\n\u001b[0;32m--> 410\u001b[0;31m                                 name=name)\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m   def compute_gradients(self, loss, var_list=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tenv/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, global_step, name)\u001b[0m\n\u001b[1;32m    608\u001b[0m           \u001b[0mscope_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"update_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mscope_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m           \u001b[0mupdate_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0mapply_updates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tenv/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mupdate_op\u001b[0;34m(self, optimizer, g)\u001b[0m\n\u001b[1;32m    113\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mupdate_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m       \u001b[0mupdate_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstraint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tenv/lib/python3.6/site-packages/tensorflow/python/training/adam.py\u001b[0m in \u001b[0;36m_apply_dense\u001b[0;34m(self, grad, var)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_beta2_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epsilon_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         grad, use_locking=self._use_locking).op\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_resource_apply_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tenv/lib/python3.6/site-packages/tensorflow/python/training/gen_training_ops.py\u001b[0m in \u001b[0;36mapply_adam\u001b[0;34m(var, m, v, beta1_power, beta2_power, lr, beta1, beta2, epsilon, grad, use_locking, use_nesterov, name)\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0mbeta2_power\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeta2_power\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_locking\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         use_nesterov=use_nesterov, name=name)\n\u001b[0m\u001b[1;32m    295\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tenv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tenv/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m                 instructions)\n\u001b[0;32m--> 488\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[1;32m    490\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m~/anaconda3/envs/tenv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3272\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3273\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3274\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3276\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tenv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1790\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1791\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1792\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m     \u001b[0;31m# Initialize self._outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tenv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape must be rank 0 but is rank 1 for 'Adam_discriminator/update_dis/w1_dis/ApplyAdam' (op: 'ApplyAdam') with input shapes: [10,7], [10,7], [10,7], [], [], [1], [], [], [], [10,7]."
     ]
    }
   ],
   "source": [
    "## Main\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
