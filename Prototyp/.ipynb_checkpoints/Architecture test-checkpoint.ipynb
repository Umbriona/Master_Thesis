{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "## imports\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import models.component as cP\n",
    "import models.data_process as dP\n",
    "import math\n",
    "\n",
    "from multiprocessing import Process\n",
    "\n",
    "# Plot Data\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global\n",
    "EPOCH = 100\n",
    "fps_size = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_class1():\n",
    "    \n",
    "    fps_dict = dP.process_data_np(ds_name = 'LD50_Test_data', pool_size = 4)\n",
    "\n",
    "    fps1_train = fps_dict['fps1_train_np']\n",
    "    fps1_test = fps_dict['fps1_test_np']\n",
    "    labels1_train = fps_dict['labels1_train_np']\n",
    "    labels1_test = fps_dict['labels1_test_np']\n",
    "    i_lr = 0.005\n",
    "    lastLoss = 0\n",
    "    layers_dim = np.array([256, 64, 10], dtype = np.float32)\n",
    "    fps_dim = 2048\n",
    "    batch_size = 64\n",
    "    with tf.device('/gpu:1'):\n",
    "\n",
    "        labels = tf.placeholder(tf.float32, shape=[None, 10], name = 'labels')\n",
    "        is_train_enc = tf.placeholder(tf.bool, name = 'is_train_enc')\n",
    "        fps = tf.placeholder(tf.float32, shape = [None, 2048,2], name ='fps')\n",
    "        lr_1 = tf.placeholder(tf.float32, name = 'lr_1')\n",
    "        \n",
    "        predict, _ = cP.cnn_encoder(fps, fps_dim, layers_dim, is_train_enc, reuse=False)\n",
    "        classifyer1_loss = tf.losses.softmax_cross_entropy(labels,predict)\n",
    "        \n",
    "        predicted_labels = tf.one_hot(tf.math.argmax(predict, axis = 1), 10, on_value = 1.0, off_value = 0.0)\n",
    "        classifyer1_acc = tf.metrics.accuracy(labels,predicted_labels)\n",
    "        \n",
    "        t_vars = tf.trainable_variables()\n",
    "        enc_vars = [var for var in t_vars if 'encode' in var.name]\n",
    "        \n",
    "        trainer_enc = tf.train.AdamOptimizer(learning_rate =lr_1, beta1=0.9, beta2=0.999,\n",
    "                                             epsilon=1e-08, use_locking=False,\n",
    "                                             name='Adam_encoder').minimize(classifyer1_loss, var_list=enc_vars)\n",
    "        \n",
    "\n",
    "        \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    num_batch_epoch = math.floor(len(fps1_train)/batch_size)\n",
    "\n",
    "    for i in range(EPOCH):\n",
    "        batch1_train = dP.batch_gen(fps1_train, labels1_train, batch_size = batch_size, FLAG = True)\n",
    "        batch1_val = dP.batch_gen(fps1_test, labels1_test, batch_size = batch_size, FLAG = True) \n",
    "\n",
    "        for j in range(num_batch_epoch):\n",
    "\n",
    "            dict_train =  {fps: batch1_train['fps'][j], labels: batch1_train['label'][j], is_train_enc: True, lr_1: i_lr}\n",
    "            sess.run([trainer_enc], feed_dict = dict_train)\n",
    "\n",
    "        dict_train_loss = {fps: fps1_train, labels: labels1_train, is_train_enc: False}\n",
    "        trainLoss, trainAcc = sess.run([classifyer1_loss, classifyer1_acc], feed_dict = dict_train_loss)\n",
    "        if i%5 == 0 and i != 0:\n",
    "            if lastLoss - trainLoss <= 0.00000:\n",
    "                i_lr = max([i_lr * 0.5, 0.00001])\n",
    "            lastLoss = trainLoss \n",
    "        dict_val =  {fps: fps1_test, labels: labels1_test, is_train_enc: False}\n",
    "        valLoss, valAcc = sess.run([classifyer1_loss, classifyer1_acc], feed_dict = dict_val)\n",
    "        #encoder_dict={fps: fps_test, is_train_enc: False}\n",
    "        #print(sess.run(predict, feed_dict = encoder_dict))\n",
    "        \n",
    "        print('1 %f trainLoss = %f valLoss = %f trainAcc = %f valAcc = %f' % (i_lr, trainLoss, valLoss, trainAcc[0], valAcc[0]))\n",
    "    sess.close()\n",
    "\n",
    "def train_class2():\n",
    "    fps_dict = dP.process_data_np(ds_name = 'LD50_Test_data', pool_size = 4)\n",
    "\n",
    "    fps_train = fps_dict['fps2_train_np']\n",
    "    fps_test = fps_dict['fps2_test_np']\n",
    "    labels_train = fps_dict['labels2_train_np']\n",
    "    labels_test = fps_dict['labels2_test_np']\n",
    "    i_lr = 0.01\n",
    "    lastLoss = 0\n",
    "    layers_dim = np.array([256, 64, 10], dtype = np.float32)\n",
    "    fps_dim = 512\n",
    "    batch_size = 64\n",
    "\n",
    "        \n",
    "    with tf.device('/gpu:0'):\n",
    "        labels2 = tf.placeholder(tf.float32, shape=[None, 10], name = 'labels2')\n",
    "        is_train_dense = tf.placeholder(tf.bool, name = 'is_train_dense')\n",
    "        fps2 = tf.placeholder(tf.float32, shape = [None, 512], name ='fps2')\n",
    "        lr_2 = tf.placeholder(tf.float32, name = 'lr_2')\n",
    "        predict2 = cP.dense_encoder(fps2, 512, layers_dim, is_train_dense, reuse=False)\n",
    "        classifyer2_loss = tf.losses.softmax_cross_entropy(labels2,predict2)\n",
    "        predicted_labels = tf.one_hot(tf.math.argmax(predict2, axis = 1), 10, on_value = 1.0, off_value = 0.0)\n",
    "        classifyer2_acc = tf.metrics.accuracy(labels2,predicted_labels)\n",
    "        \n",
    "        t_vars = tf.trainable_variables()\n",
    "        dense_vars = [var for var in t_vars if 'dense_class' in var.name]\n",
    "        trainer_dense = tf.train.AdamOptimizer(learning_rate =lr_2, beta1=0.9, beta2=0.999,\n",
    "                                             epsilon=1e-08, use_locking=False,\n",
    "                                             name='Adam_encoder').minimize(classifyer2_loss, var_list=dense_vars)\n",
    "        \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    num_batch_epoch = math.floor(len(fps_train)/batch_size)\n",
    "\n",
    "    for i in range(EPOCH):\n",
    "        batch1_train = dP.batch_gen(fps_train, labels_train, batch_size = batch_size, FLAG = True)\n",
    "        batch1_val = dP.batch_gen(fps_test, labels_test, batch_size = batch_size, FLAG = True) \n",
    "\n",
    "        for j in range(num_batch_epoch):\n",
    "\n",
    "            dict_train =  {fps2: batch1_train['fps'][j], labels2: batch1_train['label'][j], is_train_dense: True, lr_2: i_lr}\n",
    "            sess.run([trainer_dense], feed_dict = dict_train)\n",
    "\n",
    "        dict_train_loss = {fps2: fps_train, labels2: labels_train, is_train_dense: False}\n",
    "        trainLoss, trainAcc = sess.run([classifyer2_loss, classifyer2_acc], feed_dict = dict_train_loss)\n",
    "        if i%5 == 0 and i != 0:\n",
    "            if lastLoss - trainLoss <= 0.00000:\n",
    "                i_lr = max([i_lr * 0.5, 0.00001])\n",
    "            lastLoss = trainLoss \n",
    "        dict_val =  {fps2: fps_test, labels2: labels_test, is_train_dense: False}\n",
    "        valLoss, valAcc = sess.run([classifyer2_loss, classifyer2_acc], feed_dict = dict_val)\n",
    "        #encoder_dict={fps: fps_test, is_train_enc: False}\n",
    "        #print(sess.run(predict, feed_dict = encoder_dict))\n",
    "        \n",
    "        print('2 %f trainLoss = %f valLoss = %f trainAcc = %f valAcc = %f' % (i_lr, trainLoss, valLoss, trainAcc[0], valAcc[0]))\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\n",
      "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\n",
      "    run_metadata)\n",
      "tensorflow.python.framework.errors_impl.InternalError: cuDNN launch failure : input shape ([64,256,1,1])\n",
      "\t [[{{node dense_class/Layer0/dense/cond/FusedBatchNorm}} = FusedBatchNorm[T=DT_FLOAT, data_format=\"NCHW\", epsilon=1.001e-05, is_training=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](gradients/dense_class/Layer0/dense/cond/FusedBatchNorm_grad/FusedBatchNormGrad-1-TransposeNHWCToNCHW-LayoutOptimizer, dense_class/Layer0/dense/cond/FusedBatchNorm_1/Switch_1:1, dense_class/Layer0/dense/cond/FusedBatchNorm_1/Switch_2:1, dense_class/Layer2/dense/cond/Const, dense_class/Layer2/dense/cond/Const)]]\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-3-499143be41c3>\", line 106, in train_class2\n",
      "    sess.run([trainer_dense], feed_dict = dict_train)\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 929, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1152, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\n",
      "    run_metadata)\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\n",
      "    raise type(e)(node_def, op, message)\n",
      "tensorflow.python.framework.errors_impl.InternalError: cuDNN launch failure : input shape ([64,256,1,1])\n",
      "\t [[node dense_class/Layer0/dense/cond/FusedBatchNorm (defined at /home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py:368)  = FusedBatchNorm[T=DT_FLOAT, data_format=\"NCHW\", epsilon=1.001e-05, is_training=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](gradients/dense_class/Layer0/dense/cond/FusedBatchNorm_grad/FusedBatchNormGrad-1-TransposeNHWCToNCHW-LayoutOptimizer, dense_class/Layer0/dense/cond/FusedBatchNorm_1/Switch_1:1, dense_class/Layer0/dense/cond/FusedBatchNorm_1/Switch_2:1, dense_class/Layer2/dense/cond/Const, dense_class/Layer2/dense/cond/Const)]]\n",
      "\n",
      "Caused by op 'dense_class/Layer0/dense/cond/FusedBatchNorm', defined at:\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/asyncio/base_events.py\", line 438, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/asyncio/base_events.py\", line 1451, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/asyncio/events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n",
      "    self.run()\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n",
      "    yield gen.maybe_future(dispatch(*args))\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n",
      "    yield gen.maybe_future(handler(stream, idents, msg))\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n",
      "    user_expressions, allow_stdin,\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2843, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2869, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3044, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3209, in run_ast_nodes\n",
      "    if (yield from self.run_code(code, result)):\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3291, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-4-f6e14524bc62>\", line 6, in <module>\n",
      "    Process(target=train_class2).start()\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/multiprocessing/process.py\", line 105, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/multiprocessing/context.py\", line 223, in _Popen\n",
      "    return _default_context.get_context().Process._Popen(process_obj)\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/multiprocessing/context.py\", line 277, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/multiprocessing/popen_fork.py\", line 19, in __init__\n",
      "    self._launch(process_obj)\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/multiprocessing/popen_fork.py\", line 73, in _launch\n",
      "    code = process_obj._bootstrap()\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-3-499143be41c3>\", line 83, in train_class2\n",
      "    predict2 = cP.dense_encoder(fps2, 512, layers_dim, is_train_dense, reuse=False)\n",
      "  File \"/home/sandra/Documents/Thesis/Programs/models/component.py\", line 50, in dense_encoder\n",
      "    out_layer0 = denseLayer(input, fps_dim, layers_dim[0], is_train = is_train_enc)\n",
      "  File \"/home/sandra/Documents/Thesis/Programs/models/component.py\", line 29, in denseLayer\n",
      "    updates_collections=None, scope = 'dense')\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 182, in func_with_args\n",
      "    return func(*args, **current_args)\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 596, in batch_norm\n",
      "    scope=scope)\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 382, in _fused_batch_norm\n",
      "    is_training, _fused_batch_norm_training, _fused_batch_norm_inference)\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/utils.py\", line 217, in smart_cond\n",
      "    return control_flow_ops.cond(pred, fn1, fn2, name)\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2086, in cond\n",
      "    orig_res_t, res_t = context_t.BuildCondBranch(true_fn)\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1930, in BuildCondBranch\n",
      "    original_result = fn()\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 368, in _fused_batch_norm_training\n",
      "    inputs, gamma, beta, epsilon=epsilon, data_format=data_format)\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py\", line 909, in fused_batch_norm\n",
      "    name=name)\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 3466, in _fused_batch_norm\n",
      "    is_training=is_training, name=name)\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\n",
      "    op_def=op_def)\n",
      "  File \"/home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\n",
      "    self._traceback = tf_stack.extract_stack()\n",
      "\n",
      "InternalError (see above for traceback): cuDNN launch failure : input shape ([64,256,1,1])\n",
      "\t [[node dense_class/Layer0/dense/cond/FusedBatchNorm (defined at /home/sandra/anaconda3/envs/tenv/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py:368)  = FusedBatchNorm[T=DT_FLOAT, data_format=\"NCHW\", epsilon=1.001e-05, is_training=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](gradients/dense_class/Layer0/dense/cond/FusedBatchNorm_grad/FusedBatchNormGrad-1-TransposeNHWCToNCHW-LayoutOptimizer, dense_class/Layer0/dense/cond/FusedBatchNorm_1/Switch_1:1, dense_class/Layer0/dense/cond/FusedBatchNorm_1/Switch_2:1, dense_class/Layer2/dense/cond/Const, dense_class/Layer2/dense/cond/Const)]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.030000 trainLoss = 1.691160 valLoss = 1.644524 trainAcc = 0.000000 valAcc = 0.901234\n",
      "1 0.030000 trainLoss = 1.687731 valLoss = 1.643340 trainAcc = 0.901998 valAcc = 0.901658\n",
      "1 0.030000 trainLoss = 1.687953 valLoss = 1.645295 trainAcc = 0.901998 valAcc = 0.901780\n",
      "1 0.030000 trainLoss = 1.687399 valLoss = 1.639475 trainAcc = 0.901998 valAcc = 0.901837\n",
      "1 0.030000 trainLoss = 1.688076 valLoss = 1.641042 trainAcc = 0.901998 valAcc = 0.901871\n",
      "1 0.015000 trainLoss = 1.690066 valLoss = 1.647867 trainAcc = 0.901998 valAcc = 0.901893\n",
      "1 0.015000 trainLoss = 1.687921 valLoss = 1.645222 trainAcc = 0.901998 valAcc = 0.901908\n",
      "1 0.015000 trainLoss = 1.687232 valLoss = 1.644982 trainAcc = 0.901998 valAcc = 0.901920\n",
      "1 0.015000 trainLoss = 1.687449 valLoss = 1.645957 trainAcc = 0.901998 valAcc = 0.901929\n",
      "1 0.015000 trainLoss = 1.688334 valLoss = 1.643855 trainAcc = 0.901998 valAcc = 0.901936\n",
      "1 0.015000 trainLoss = 1.686667 valLoss = 1.643499 trainAcc = 0.901998 valAcc = 0.901941\n",
      "1 0.015000 trainLoss = 1.686291 valLoss = 1.644262 trainAcc = 0.901998 valAcc = 0.901946\n",
      "1 0.015000 trainLoss = 1.687494 valLoss = 1.639992 trainAcc = 0.901998 valAcc = 0.901950\n",
      "1 0.015000 trainLoss = 1.686238 valLoss = 1.638933 trainAcc = 0.901998 valAcc = 0.901954\n",
      "1 0.015000 trainLoss = 1.686788 valLoss = 1.639686 trainAcc = 0.901998 valAcc = 0.901957\n",
      "1 0.007500 trainLoss = 1.687609 valLoss = 1.644518 trainAcc = 0.901998 valAcc = 0.901959\n",
      "1 0.007500 trainLoss = 1.687084 valLoss = 1.643174 trainAcc = 0.901998 valAcc = 0.901962\n",
      "1 0.007500 trainLoss = 1.686148 valLoss = 1.642290 trainAcc = 0.901998 valAcc = 0.901964\n",
      "1 0.007500 trainLoss = 1.685723 valLoss = 1.642587 trainAcc = 0.901998 valAcc = 0.901966\n",
      "1 0.007500 trainLoss = 1.686753 valLoss = 1.644774 trainAcc = 0.901998 valAcc = 0.901967\n",
      "1 0.007500 trainLoss = 1.686154 valLoss = 1.642921 trainAcc = 0.901998 valAcc = 0.901969\n",
      "1 0.007500 trainLoss = 1.685712 valLoss = 1.642483 trainAcc = 0.901998 valAcc = 0.901970\n",
      "1 0.007500 trainLoss = 1.685674 valLoss = 1.643269 trainAcc = 0.901998 valAcc = 0.901971\n",
      "1 0.007500 trainLoss = 1.686711 valLoss = 1.642868 trainAcc = 0.901998 valAcc = 0.901972\n",
      "1 0.007500 trainLoss = 1.685768 valLoss = 1.643651 trainAcc = 0.901998 valAcc = 0.901973\n",
      "1 0.003750 trainLoss = 1.686275 valLoss = 1.644891 trainAcc = 0.901998 valAcc = 0.901974\n",
      "1 0.003750 trainLoss = 1.685745 valLoss = 1.643804 trainAcc = 0.901998 valAcc = 0.901975\n",
      "1 0.003750 trainLoss = 1.685468 valLoss = 1.643383 trainAcc = 0.901998 valAcc = 0.901976\n",
      "1 0.003750 trainLoss = 1.685479 valLoss = 1.642851 trainAcc = 0.901998 valAcc = 0.901977\n",
      "1 0.003750 trainLoss = 1.685860 valLoss = 1.645141 trainAcc = 0.901998 valAcc = 0.901978\n",
      "1 0.003750 trainLoss = 1.686120 valLoss = 1.645975 trainAcc = 0.901998 valAcc = 0.901978\n",
      "1 0.003750 trainLoss = 1.685490 valLoss = 1.644621 trainAcc = 0.901998 valAcc = 0.901979\n",
      "1 0.003750 trainLoss = 1.685383 valLoss = 1.644670 trainAcc = 0.901998 valAcc = 0.901979\n",
      "1 0.003750 trainLoss = 1.685771 valLoss = 1.644755 trainAcc = 0.901998 valAcc = 0.901980\n",
      "1 0.003750 trainLoss = 1.685716 valLoss = 1.643568 trainAcc = 0.901998 valAcc = 0.901981\n",
      "1 0.003750 trainLoss = 1.685096 valLoss = 1.644017 trainAcc = 0.901998 valAcc = 0.901981\n",
      "1 0.003750 trainLoss = 1.682170 valLoss = 1.641996 trainAcc = 0.901998 valAcc = 0.901981\n",
      "1 0.003750 trainLoss = 1.683467 valLoss = 1.642283 trainAcc = 0.901998 valAcc = 0.901982\n",
      "1 0.003750 trainLoss = 1.683830 valLoss = 1.641534 trainAcc = 0.901998 valAcc = 0.901982\n",
      "1 0.003750 trainLoss = 1.684303 valLoss = 1.641448 trainAcc = 0.901998 valAcc = 0.901983\n",
      "1 0.003750 trainLoss = 1.679896 valLoss = 1.638982 trainAcc = 0.901998 valAcc = 0.901983\n",
      "1 0.003750 trainLoss = 1.685359 valLoss = 1.641144 trainAcc = 0.901998 valAcc = 0.901983\n",
      "1 0.003750 trainLoss = 1.683733 valLoss = 1.645170 trainAcc = 0.901998 valAcc = 0.901984\n",
      "1 0.003750 trainLoss = 1.680125 valLoss = 1.638373 trainAcc = 0.901998 valAcc = 0.901984\n",
      "1 0.003750 trainLoss = 1.677489 valLoss = 1.637328 trainAcc = 0.901998 valAcc = 0.901984\n",
      "1 0.003750 trainLoss = 1.677240 valLoss = 1.638362 trainAcc = 0.901998 valAcc = 0.901985\n",
      "1 0.003750 trainLoss = 1.678721 valLoss = 1.638498 trainAcc = 0.901998 valAcc = 0.901985\n",
      "1 0.003750 trainLoss = 1.679040 valLoss = 1.637120 trainAcc = 0.901998 valAcc = 0.901985\n",
      "1 0.003750 trainLoss = 1.676397 valLoss = 1.637208 trainAcc = 0.901998 valAcc = 0.901986\n",
      "1 0.003750 trainLoss = 1.676425 valLoss = 1.635895 trainAcc = 0.901998 valAcc = 0.901986\n",
      "1 0.003750 trainLoss = 1.677160 valLoss = 1.638318 trainAcc = 0.901998 valAcc = 0.901986\n",
      "1 0.003750 trainLoss = 1.676703 valLoss = 1.636248 trainAcc = 0.901998 valAcc = 0.901986\n",
      "1 0.003750 trainLoss = 1.679139 valLoss = 1.638530 trainAcc = 0.901998 valAcc = 0.901986\n",
      "1 0.003750 trainLoss = 1.675700 valLoss = 1.635014 trainAcc = 0.901998 valAcc = 0.901987\n",
      "1 0.003750 trainLoss = 1.677816 valLoss = 1.637225 trainAcc = 0.901998 valAcc = 0.901987\n",
      "1 0.003750 trainLoss = 1.676319 valLoss = 1.639117 trainAcc = 0.901998 valAcc = 0.901987\n",
      "1 0.003750 trainLoss = 1.679356 valLoss = 1.639688 trainAcc = 0.901998 valAcc = 0.901987\n",
      "1 0.003750 trainLoss = 1.681411 valLoss = 1.640217 trainAcc = 0.901998 valAcc = 0.901987\n",
      "1 0.003750 trainLoss = 1.676501 valLoss = 1.636315 trainAcc = 0.901998 valAcc = 0.901988\n",
      "1 0.003750 trainLoss = 1.678146 valLoss = 1.637918 trainAcc = 0.901998 valAcc = 0.901988\n",
      "1 0.003750 trainLoss = 1.675961 valLoss = 1.635085 trainAcc = 0.901998 valAcc = 0.901988\n",
      "1 0.003750 trainLoss = 1.678490 valLoss = 1.638557 trainAcc = 0.901998 valAcc = 0.901988\n",
      "1 0.003750 trainLoss = 1.676064 valLoss = 1.634495 trainAcc = 0.901998 valAcc = 0.901988\n",
      "1 0.003750 trainLoss = 1.676249 valLoss = 1.636500 trainAcc = 0.901998 valAcc = 0.901989\n",
      "1 0.003750 trainLoss = 1.676802 valLoss = 1.637719 trainAcc = 0.901998 valAcc = 0.901989\n",
      "1 0.001875 trainLoss = 1.714226 valLoss = 1.666718 trainAcc = 0.901998 valAcc = 0.901982\n",
      "1 0.001875 trainLoss = 1.675283 valLoss = 1.634908 trainAcc = 0.901990 valAcc = 0.901981\n",
      "1 0.001875 trainLoss = 1.675249 valLoss = 1.635698 trainAcc = 0.901991 valAcc = 0.901982\n",
      "1 0.001875 trainLoss = 1.674245 valLoss = 1.634034 trainAcc = 0.901991 valAcc = 0.901982\n",
      "1 0.001875 trainLoss = 1.674335 valLoss = 1.635391 trainAcc = 0.901991 valAcc = 0.901982\n",
      "1 0.001875 trainLoss = 1.674785 valLoss = 1.635458 trainAcc = 0.901991 valAcc = 0.901982\n",
      "1 0.001875 trainLoss = 1.673927 valLoss = 1.634220 trainAcc = 0.901991 valAcc = 0.901983\n",
      "1 0.001875 trainLoss = 1.673843 valLoss = 1.633607 trainAcc = 0.901991 valAcc = 0.901983\n",
      "1 0.001875 trainLoss = 1.674221 valLoss = 1.633720 trainAcc = 0.901991 valAcc = 0.901983\n",
      "1 0.001875 trainLoss = 1.674966 valLoss = 1.634742 trainAcc = 0.901991 valAcc = 0.901983\n",
      "1 0.001875 trainLoss = 1.673409 valLoss = 1.634154 trainAcc = 0.901991 valAcc = 0.901983\n",
      "1 0.001875 trainLoss = 1.670630 valLoss = 1.632928 trainAcc = 0.901991 valAcc = 0.901984\n",
      "1 0.001875 trainLoss = 1.669085 valLoss = 1.629791 trainAcc = 0.901991 valAcc = 0.901984\n",
      "1 0.001875 trainLoss = 1.668273 valLoss = 1.628477 trainAcc = 0.901992 valAcc = 0.901984\n",
      "1 0.001875 trainLoss = 1.664555 valLoss = 1.625332 trainAcc = 0.901992 valAcc = 0.901984\n",
      "1 0.001875 trainLoss = 1.668037 valLoss = 1.629617 trainAcc = 0.901992 valAcc = 0.901984\n",
      "1 0.001875 trainLoss = 1.677847 valLoss = 1.644863 trainAcc = 0.901992 valAcc = 0.901981\n",
      "1 0.001875 trainLoss = 1.674198 valLoss = 1.641356 trainAcc = 0.901988 valAcc = 0.901980\n",
      "1 0.001875 trainLoss = 1.668189 valLoss = 1.628926 trainAcc = 0.901986 valAcc = 0.901979\n",
      "1 0.001875 trainLoss = 1.665679 valLoss = 1.626375 trainAcc = 0.901986 valAcc = 0.901979\n",
      "1 0.001875 trainLoss = 1.665476 valLoss = 1.630150 trainAcc = 0.901986 valAcc = 0.901979\n",
      "1 0.001875 trainLoss = 1.667681 valLoss = 1.629749 trainAcc = 0.901987 valAcc = 0.901980\n",
      "1 0.001875 trainLoss = 1.659367 valLoss = 1.625490 trainAcc = 0.901987 valAcc = 0.901980\n",
      "1 0.001875 trainLoss = 1.659421 valLoss = 1.624575 trainAcc = 0.901988 valAcc = 0.901983\n",
      "1 0.001875 trainLoss = 1.655628 valLoss = 1.625492 trainAcc = 0.901990 valAcc = 0.901985\n",
      "1 0.001875 trainLoss = 1.662169 valLoss = 1.630066 trainAcc = 0.901992 valAcc = 0.901988\n",
      "1 0.001875 trainLoss = 1.663536 valLoss = 1.624313 trainAcc = 0.901995 valAcc = 0.901989\n",
      "1 0.001875 trainLoss = 1.652923 valLoss = 1.622881 trainAcc = 0.901995 valAcc = 0.901991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.001875 trainLoss = 1.659947 valLoss = 1.634398 trainAcc = 0.901999 valAcc = 0.901995\n",
      "1 0.001875 trainLoss = 1.654985 valLoss = 1.624540 trainAcc = 0.902002 valAcc = 0.901996\n",
      "1 0.001875 trainLoss = 1.647781 valLoss = 1.625780 trainAcc = 0.902002 valAcc = 0.902002\n",
      "1 0.001875 trainLoss = 1.649877 valLoss = 1.626079 trainAcc = 0.902007 valAcc = 0.902007\n",
      "1 0.001875 trainLoss = 1.643970 valLoss = 1.627171 trainAcc = 0.902013 valAcc = 0.902016\n",
      "1 0.001875 trainLoss = 1.647217 valLoss = 1.626443 trainAcc = 0.902023 valAcc = 0.902024\n",
      "1 0.001875 trainLoss = 1.641789 valLoss = 1.628338 trainAcc = 0.902029 valAcc = 0.902033\n"
     ]
    }
   ],
   "source": [
    "## Main\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    Process(target=train_class1).start()\n",
    "    Process(target=train_class2).start()\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
